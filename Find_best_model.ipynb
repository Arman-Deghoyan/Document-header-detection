{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc601f45",
   "metadata": {},
   "source": [
    "### The process of finding best model:\n",
    "\n",
    "1. Parse and read the documents splitting first 13000 documents randomly on train and validation datasets. The last 1932 documents I have left as unseen test data.\n",
    "2. The train, validation, test dataframes are with two columns row and label. Where row column values are each row list of strings for each document and label each label for current row.\n",
    "3. Label column is mapped to 0,1 in order to make the task a binary classification problem.\n",
    "4. The row column passes three main preprocessing steps after various experimentations. First the duplicated whitespaces are removed and list of strings are joined with one whitespace. Second the punctuation is removed. Third stopwords with the help of spacy library are removed.\n",
    "5. For training I have experimented various combinations of models. For tokenization I have used Count Vectorizer and TfIdf. For ML classification algorithms I have used Naive Bayes, Logistic Regression. Finally I have trained facebook's famous fasttext model which gave slightly better results from all the model combinations.\n",
    "6. Two best models were tfidf combibation with Naive Bayes and fasttext and I have trained, saved those two models after splitting on train, validation, test datasets. The names for those models are nb_model.pkl and fasttext_model.bin. The TfIDF/ Naise Baes combination trained on all the 14932 is saved as model.pkl file.\n",
    "7. The models are evaluated on unseen test datasets ( on last 1932 documents ). The main metrics used for classification are accuracy, F1 score of imbalanced HEADER label and ROC AUC score. \n",
    "8. After experimentation in Jupyter Notebook i have created a python package where you can with cli commands run training, prediction and classification summary options. All below examples are given of training, predicting and summarizing Naive Bayes model with TFIDF tokenization. Example of Training:\n",
    "\n",
    "    `python run_script.py -t \"{path_to_training_dataset.txt}\"`\n",
    "    \n",
    "\n",
    "9. Example of prediction. Make sure you have the model file, for example, nb_model.pkl in the package directory where the run_script.py is located. The prediction results are saved in headers_prediction_results.csv where the csv file contains three columns document, header_rows and header_count. Header_rows are rows where model labelem them as HEADER. Header count is how many headers does the current document contain. \n",
    "\n",
    "    `python run_script.py -p \"{path_to_test_dataset.txt}\"`\n",
    "    \n",
    "    \n",
    "10. Example of classification summarization. Make sure you have the model file, for example, nb_model.pkl in the package directory where the run_script.py is located. There is an example Results_on_test_data.PNG, which is screenshot of terminal from results of Naive Bayes model on unseen test dataset (1932 documents).\n",
    "\n",
    "    `python run_script.py -s \"{path_to_test_dataset.txt}\"`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d95ec",
   "metadata": {},
   "source": [
    "# Train, validation and test whole training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57aa31fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from fasttext import train_supervised\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8293c1",
   "metadata": {},
   "source": [
    "# Reading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e933a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../document-standardization-training-dataset.txt', encoding=\"utf8\") as f:\n",
    "    lines = [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e55d286",
   "metadata": {},
   "source": [
    "# Constructing random train and validation dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4924a6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13000/13000 [01:31<00:00, 141.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1683648\n",
      "1683648\n",
      "417593\n",
      "417593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_raw_values = []\n",
    "train_labels = []\n",
    "\n",
    "validation_raw_values = []\n",
    "validation_labels = []\n",
    "\n",
    "train_index = 0\n",
    "validation_index = 0\n",
    "\n",
    "# taking first 13000 documents and leaving 1932 for test \n",
    "for document in tqdm(lines[:13000]):\n",
    "    \n",
    "    # generating random number for random train and validation split of documents\n",
    "    random_number = np.random.random()\n",
    "    \n",
    "    # making the training size around 20%\n",
    "    if random_number > 0.20:\n",
    "        for line_dict in eval(document):\n",
    "\n",
    "            train_labels.append(line_dict['type'])\n",
    "            train_raw_values.append([])\n",
    "\n",
    "            for value in line_dict['values']:\n",
    "                train_raw_values[train_index].append(value['value'])\n",
    "\n",
    "            train_index+=1\n",
    "            \n",
    "    # making the validation size around 20%\n",
    "    else:\n",
    "        for line_dict in eval(document):\n",
    "            \n",
    "            validation_labels.append(line_dict['type'])\n",
    "            validation_raw_values.append([])\n",
    "\n",
    "            for value in line_dict['values']:\n",
    "                validation_raw_values[validation_index].append(value['value'])\n",
    "\n",
    "            validation_index+=1\n",
    "\n",
    "print(len(train_raw_values))\n",
    "print(len(train_labels))\n",
    "print(len(validation_raw_values))\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e32567",
   "metadata": {},
   "source": [
    "# Constructing the unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "469f1ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1932/1932 [00:15<00:00, 122.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414981\n",
      "414981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "values = []\n",
    "labels = []\n",
    "index = 0\n",
    "\n",
    "# leaving last 1932 documents for test \n",
    "for document in tqdm(lines[13000:]):\n",
    "    for line_dict in eval(document):\n",
    "\n",
    "        labels.append(line_dict['type'])\n",
    "        values.append([])\n",
    "\n",
    "        for value in line_dict['values']:\n",
    "            values[index].append(value['value'])\n",
    "\n",
    "        index+=1\n",
    "\n",
    "print(len(labels))\n",
    "print(len(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec1439",
   "metadata": {},
   "source": [
    "# Getting all train, validation and test dataframes with row and label columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b68128a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1683648, 2)\n",
      "(417593, 2)\n",
      "(414981, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame({'row': train_raw_values, 'label': train_labels})\n",
    "df_validation = pd.DataFrame({'row': validation_raw_values, 'label': validation_labels})\n",
    "df_test = pd.DataFrame({'row': values, 'label': labels})\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_validation.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "482fd3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Country Estates (cene), , , , , , , , , , , ,...</td>\n",
       "      <td>NO_TYPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Statement (12 months), , , , , , , , , , , , , ]</td>\n",
       "      <td>NO_TYPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Period = Jan 2016-0ec 2016, , , , , , , , , ,...</td>\n",
       "      <td>NO_TYPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Book = Cash, , , , , , , , , , , , , ]</td>\n",
       "      <td>NO_TYPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[, Jan 2016, ? Feb 2016, ? Mar 2016, 0 Apr 201...</td>\n",
       "      <td>HEADERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417588</th>\n",
       "      <td>[Elevator Maintenance, 0.00, 0.00, 1007.50, 10...</td>\n",
       "      <td>EXPENSES_MAINTENANCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417589</th>\n",
       "      <td>[Total Service Related Expenses, 4838.58, 3488...</td>\n",
       "      <td>TOTALS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417590</th>\n",
       "      <td>[Total Operating Expenses, 78485.28, 95088.67,...</td>\n",
       "      <td>TOTALS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417591</th>\n",
       "      <td>[Net Operating Income (Loss), 56260.73, 44528....</td>\n",
       "      <td>TOTALS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417592</th>\n",
       "      <td>[Other, , , , , , , , , , , , , , ]</td>\n",
       "      <td>OTHER_START</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>417593 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      row  \\\n",
       "0       [Country Estates (cene), , , , , , , , , , , ,...   \n",
       "1       [Statement (12 months), , , , , , , , , , , , , ]   \n",
       "2       [Period = Jan 2016-0ec 2016, , , , , , , , , ,...   \n",
       "3                 [Book = Cash, , , , , , , , , , , , , ]   \n",
       "4       [, Jan 2016, ? Feb 2016, ? Mar 2016, 0 Apr 201...   \n",
       "...                                                   ...   \n",
       "417588  [Elevator Maintenance, 0.00, 0.00, 1007.50, 10...   \n",
       "417589  [Total Service Related Expenses, 4838.58, 3488...   \n",
       "417590  [Total Operating Expenses, 78485.28, 95088.67,...   \n",
       "417591  [Net Operating Income (Loss), 56260.73, 44528....   \n",
       "417592                [Other, , , , , , , , , , , , , , ]   \n",
       "\n",
       "                       label  \n",
       "0                    NO_TYPE  \n",
       "1                    NO_TYPE  \n",
       "2                    NO_TYPE  \n",
       "3                    NO_TYPE  \n",
       "4                    HEADERS  \n",
       "...                      ...  \n",
       "417588  EXPENSES_MAINTENANCE  \n",
       "417589                TOTALS  \n",
       "417590                TOTALS  \n",
       "417591                TOTALS  \n",
       "417592           OTHER_START  \n",
       "\n",
       "[417593 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b462188",
   "metadata": {},
   "source": [
    "# Applying preprocessing to independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acda761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1683648, 2)\n",
      "(417593, 2)\n",
      "(414981, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train['row'] = df_train['row'].apply(lambda x: \" \".join((\" \".join(x)).split()))\n",
    "df_validation['row'] = df_validation['row'].apply(lambda x: \" \".join((\" \".join(x)).split()))\n",
    "df_test['row'] = df_test['row'].apply(lambda x: \" \".join((\" \".join(x)).split()))\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_validation.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4cbfa9",
   "metadata": {},
   "source": [
    "# Mapping label to binary 0 and 1, to solve binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8030452d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1630941\n",
       "1      52707\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'] = np.where(df_train['label'] == \"HEADERS\", 1, 0)\n",
    "df_validation['label'] = np.where(df_validation['label'] == \"HEADERS\", 1, 0)\n",
    "df_test['label'] = np.where(df_test['label'] == \"HEADERS\", 1, 0)\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f63c666e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    404800\n",
       "1     12793\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceac6388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    405921\n",
       "1      9060\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6b219",
   "metadata": {},
   "source": [
    "# Text NLP Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d41c2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from html import unescape\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "try:\n",
    "    spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    os.system('python -m spacy download en_core_web_sm')\n",
    "    spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "stops_spacy = sorted(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "stops_spacy.extend([\"is\", \"to\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d5e7b",
   "metadata": {},
   "source": [
    "# Define all auxiliary functions for text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd9166e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):  \n",
    "    text = ''.join([char if char.isalnum() or char == ' ' else ' ' for char in text])\n",
    "    text = ' '.join(text.split())  # remove multiple whitespace   \n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords_spacy(text, stopwords=stops_spacy):\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486361c0",
   "metadata": {},
   "source": [
    "# Apply all the text preprocessings to row column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1d53251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train preprocessing done\n",
      "Validation preprocessin done\n",
      "Test preprocessing done\n"
     ]
    }
   ],
   "source": [
    "df_train[\"row\"] = df_train[\"row\"].apply(remove_punctuation)\n",
    "df_train[\"row\"] = df_train[\"row\"].apply(remove_stopwords_spacy)\n",
    "print('Train preprocessing done')\n",
    "\n",
    "df_validation[\"row\"] = df_validation[\"row\"].apply(remove_punctuation)\n",
    "df_validation[\"row\"] = df_validation[\"row\"].apply(remove_stopwords_spacy)\n",
    "print('Validation preprocessin done')\n",
    "\n",
    "df_test[\"row\"] = df_test[\"row\"].apply(remove_punctuation)\n",
    "df_test[\"row\"] = df_test[\"row\"].apply(remove_stopwords_spacy)\n",
    "print('Test preprocessing done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad28bd4",
   "metadata": {},
   "source": [
    "## Split the data into X, y ( train, validation, test) and cast to numpy arrays for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c526d322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1683648,)\n",
      "(417593,)\n",
      "(414981,)\n",
      "(1683648,)\n",
      "(417593,)\n",
      "(414981,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(df_train['row'])\n",
    "X_val = np.array(df_validation['row'])\n",
    "X_test = np.array(df_test['row'])\n",
    "\n",
    "y_train = np.array(df_train['label'])\n",
    "y_val = np.array(df_validation['label'])\n",
    "y_test = np.array(df_test['label'])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0febdd48",
   "metadata": {},
   "source": [
    "# Checking count vectorizer dimension shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb1ab265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1683648, 408540)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd273b7b",
   "metadata": {},
   "source": [
    "# Training Count Vectorizer and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc617767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.974\n",
      "val accuracy 0.975\n",
      "[[400415   8850]\n",
      " [  1845  11578]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99    409265\n",
      "           1       0.57      0.86      0.68     13423\n",
      "\n",
      "    accuracy                           0.97    422688\n",
      "   macro avg       0.78      0.92      0.84    422688\n",
      "weighted avg       0.98      0.97      0.98    422688\n",
      "\n",
      "ROC AUC Score is\n",
      "\n",
      "0.9875837279186237\n"
     ]
    }
   ],
   "source": [
    "nb = Pipeline([('countVec', CountVectorizer()),\n",
    "               ('clf', MultinomialNB()),])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_val)\n",
    "y_pred_prb = nb.predict_proba(X_val)\n",
    "\n",
    "train_score = round(accuracy_score(nb.predict(X_train), y_train), 3)\n",
    "val_score = round(accuracy_score(y_pred, y_val), 3)\n",
    "\n",
    "print(f'train accuracy {train_score}')\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "print(metrics.confusion_matrix(y_val, y_pred))\n",
    "print(metrics.classification_report(y_val, y_pred))\n",
    "print('ROC AUC Score is' + '\\n')\n",
    "print(metrics.roc_auc_score(y_val, y_pred_prb[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d0c5a",
   "metadata": {},
   "source": [
    "# Training tfidf with naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6d56459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.99\n",
      "val accuracy 0.991\n",
      "[[405783   1355]\n",
      " [  2607  10196]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    407138\n",
      "           1       0.88      0.80      0.84     12803\n",
      "\n",
      "    accuracy                           0.99    419941\n",
      "   macro avg       0.94      0.90      0.92    419941\n",
      "weighted avg       0.99      0.99      0.99    419941\n",
      "\n",
      "ROC AUC Score is\n",
      "\n",
      "0.9909783414346139\n"
     ]
    }
   ],
   "source": [
    "nb = Pipeline([('tfidf', TfidfVectorizer(lowercase=False, token_pattern='\\w+', ngram_range=(1, 2), \n",
    "                                         min_df=3)),\n",
    "               ('clf', MultinomialNB()),])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_val)\n",
    "y_pred_prb = nb.predict_proba(X_val)\n",
    "\n",
    "train_score = round(accuracy_score(nb.predict(X_train), y_train), 3)\n",
    "val_score = round(accuracy_score(y_pred, y_val), 3)\n",
    "\n",
    "print(f'train accuracy {train_score}')\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "print(metrics.confusion_matrix(y_val, y_pred))\n",
    "print(metrics.classification_report(y_val, y_pred))\n",
    "\n",
    "print('ROC AUC Score is' + '\\n')\n",
    "print(metrics.roc_auc_score(y_val, y_pred_prb[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb56f8c2",
   "metadata": {},
   "source": [
    "# Training logistic regression with Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9d3d215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.989\n",
      "val accuracy 0.989\n",
      "[[403707   1093]\n",
      " [  3709   9084]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    404800\n",
      "           1       0.89      0.71      0.79     12793\n",
      "\n",
      "    accuracy                           0.99    417593\n",
      "   macro avg       0.94      0.85      0.89    417593\n",
      "weighted avg       0.99      0.99      0.99    417593\n",
      "\n",
      "ROC AUC Score is\n",
      "\n",
      "0.9926780873904608\n"
     ]
    }
   ],
   "source": [
    "logreg = Pipeline([('countVec', CountVectorizer()),\n",
    "                   ('clf', LogisticRegression(solver='liblinear'))])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_val)\n",
    "y_pred_prb = logreg.predict_proba(X_val)\n",
    "\n",
    "train_score = round(accuracy_score(y_train, logreg.predict(X_train)), 3)\n",
    "val_score = round(accuracy_score(y_val, y_pred), 3)\n",
    "\n",
    "print(f'train accuracy {train_score}')\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "print(metrics.confusion_matrix(y_val, y_pred))\n",
    "print(metrics.classification_report(y_val, y_pred))\n",
    "\n",
    "print('ROC AUC Score is' + '\\n')\n",
    "print(metrics.roc_auc_score(y_val, y_pred_prb[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4877e7ce",
   "metadata": {},
   "source": [
    "# Training facebook's fasttext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1655153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_fasttext_format(data: list, labels: list, save_path: str=None):\n",
    "    ft_data = []\n",
    "    for d, l in zip(data, labels):\n",
    "        ft_data.append(\"__label__{} {}\".format(l, d))\n",
    "    if save_path:\n",
    "        np.savetxt(save_path, ft_data, fmt='%s')\n",
    "    else:\n",
    "        return ft_data\n",
    "    \n",
    "def train_fasttext(X_train, y_train, wordNgrams=1, minCount=1, ft_train_path=\"./tmp_train.txt\", **kwargs):\n",
    "    \n",
    "    to_fasttext_format(X_train, y_train, save_path=ft_train_path)\n",
    "    ft_model = train_supervised(ft_train_path, wordNgrams=wordNgrams, minCount=minCount, epoch=10, loss=\"softmax\",  **kwargs)\n",
    "    train_preds = [i[0].split('_')[-1] for i in ft_model.predict(list(X_train))[0]]\n",
    "\n",
    "    train_score = round(accuracy_score(np.array(train_preds).astype(np.integer), y_train), 3)\n",
    "    print(f'train accuracy {train_score}')\n",
    "    \n",
    "    return ft_model, train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdf4c882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.992\n",
      "val accuracy 0.992\n",
      "[[405908   1230]\n",
      " [  2201  10602]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    407138\n",
      "           1       0.90      0.83      0.86     12803\n",
      "\n",
      "    accuracy                           0.99    419941\n",
      "   macro avg       0.95      0.91      0.93    419941\n",
      "weighted avg       0.99      0.99      0.99    419941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ft_model, train_score = train_fasttext(X_train, y_train)\n",
    "val_preds = [i[0].split('_')[-1] for i in ft_model.predict(list(X_val))[0]]\n",
    "\n",
    "val_score = round(accuracy_score(y_val, np.array(val_preds).astype(np.integer)), 3)\n",
    "\n",
    "print(f'val accuracy {val_score}')\n",
    "\n",
    "print(metrics.confusion_matrix(y_val, np.array(val_preds).astype(np.integer)))\n",
    "print(metrics.classification_report(y_val, np.array(val_preds).astype(np.integer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea2eb7",
   "metadata": {},
   "source": [
    "# Saving the fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1e4d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.save_model(\"fasttext_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5ae8ab",
   "metadata": {},
   "source": [
    "# Saving the Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b97f639b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./nb_model.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(nb, \"./nb_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7f4c0",
   "metadata": {},
   "source": [
    "# Testing on unseen test data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfe8e73",
   "metadata": {},
   "source": [
    "# Lets first test bad model naive baise on unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4dafdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.995\n",
      "[[405638    283]\n",
      " [  1921   7139]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    405921\n",
      "           1       0.96      0.79      0.87      9060\n",
      "\n",
      "    accuracy                           0.99    414981\n",
      "   macro avg       0.98      0.89      0.93    414981\n",
      "weighted avg       0.99      0.99      0.99    414981\n",
      "\n",
      "ROC AUC Score is\n",
      "\n",
      "0.9931143129379241\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb.predict(X_test)\n",
    "y_pred_prb = nb.predict_proba(X_test)\n",
    "\n",
    "test_score = round(accuracy_score(y_test, y_pred), 3)\n",
    "print(f'test accuracy {test_score}')\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "print('ROC AUC Score is' + '\\n')\n",
    "print(metrics.roc_auc_score(y_test, y_pred_prb[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b1933",
   "metadata": {},
   "source": [
    "# The best model fasttext performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad999f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.995\n",
      "[[405478    443]\n",
      " [  1505   7555]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    405921\n",
      "           1       0.94      0.83      0.89      9060\n",
      "\n",
      "    accuracy                           1.00    414981\n",
      "   macro avg       0.97      0.92      0.94    414981\n",
      "weighted avg       1.00      1.00      1.00    414981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_preds = [i[0].split('_')[-1] for i in ft_model.predict(list(df_test['row']))[0]]\n",
    "\n",
    "test_score = round(accuracy_score(y_test, np.array(test_preds).astype(np.integer)), 3)\n",
    "print(f'test accuracy {test_score}')\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, np.array(test_preds).astype(np.integer)))\n",
    "print(metrics.classification_report(y_test, np.array(test_preds).astype(np.integer)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_transformers",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
